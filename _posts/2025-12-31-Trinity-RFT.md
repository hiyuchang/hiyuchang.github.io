---
title: 'Introducing Trinity-RFT'
layout: single
---


è¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯ä»‹ç»ä¸€ä¸ª LLM çš„ RL è®­ç»ƒæ¡†æž¶ [Trinity-RFT](https://github.com/modelscope/Trinity-RFT) ï¼Œå†…å®¹ä¸»è¦åˆ†ä¸ºæ¡†æž¶ä»‹ç»å’Œä½¿ç”¨æŒ‡å—ä¸¤éƒ¨åˆ†ã€‚æ›´è¯¦ç»†çš„ä»‹ç»å¯ä»¥å‚è€ƒ[æ–‡æ¡£](https://modelscope.github.io/Trinity-RFT/zh/main/index.html)ã€‚æ³¨ï¼šæœ¬æ–‡æ‰€æœ‰å›¾ç‰‡éƒ½æ¥è‡ª Trinity-RFT å›¢é˜Ÿ[1]ã€‚

![Trinity-RFT æž¶æž„å›¾](../../images/trinity-design.png)

## æ¡†æž¶ä»‹ç»

Trinity-RFT æ¡†æž¶çš„æ ¸å¿ƒè®¾è®¡æ˜¯ explorerã€ trainerå’Œbufferä¸‰ä¸ªæ¨¡å—è§£è€¦ä½†æ˜¯ååŒå·¥ä½œï¼Œè¿™ç§â€œä¸‰ä½ä¸€ä½“â€çš„ç»“æž„ä¹Ÿæ˜¯æ¡†æž¶åç§° Trinity çš„ç”±æ¥ã€‚ç®€å•æ¥è¯´ï¼Œexplorer è´Ÿè´£æŽ¢ç´¢ä»»åŠ¡ï¼ˆtaskï¼‰äº§ç”Ÿç»éªŒï¼ˆexperienceï¼‰ï¼Œtrainer è´Ÿè´£è®­ç»ƒæ¨¡åž‹ï¼Œbuffer è´Ÿè´£å­˜å‚¨å’Œå¤„ç†æ•°æ®ï¼ˆtask å’Œ experienceï¼‰ã€‚
è¿™ç§è§£è€¦çš„è®¾è®¡ä½¿å¾—æ¡†æž¶å…·æœ‰å¾ˆå¥½çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¹Ÿä½¿å¾— Trinity-RFT æ‹¥æœ‰äº†ä¸‹é¢ä¸¤ä¸ªâ€œbeyond normal on-policy trainingâ€çš„ç‰¹ç‚¹ï¼š

### Trainer/Explorer çš„çµæ´»è°ƒåº¦

Trinity-RFT çš„ trainer å’Œ explorer æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼Œå®ƒä»¬å„è‡ªå ä¸€éƒ¨åˆ† GPUï¼Œè¿™æ ·çš„è®¾è®¡ä½¿æˆ‘ä»¬å¯ä»¥çµæ´»è°ƒåº¦ä¸¤éƒ¨åˆ†æ¨¡å—ï¼Œtrainer ä¸å¿…ç­‰å¾… explorer äº§ç”Ÿæ–° experiences ä¹Ÿå¯ä»¥ç»§ç»­ç”¨ä¹‹å‰çš„ experiences è¿›è¡Œè®­ç»ƒï¼Œä¸€ä¸ªç®€å•çš„è®¾æƒ³æ˜¯ï¼šå½“ explorer äº§ç”Ÿ experience çš„é€Ÿåº¦ç­‰äºŽ trainer æ¶ˆè€— experience çš„é€Ÿåº¦æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å®žçŽ°æ•ˆçŽ‡æœ€å¤§åŒ–ã€‚

Trinity-RFT æä¾›äº†å¤šç§è°ƒåº¦ç­–ç•¥ï¼Œç”±ä¸€ä¸ª [synchronizer](https://modelscope.github.io/Trinity-RFT/zh/main/tutorial/synchronizer.html) è´Ÿè´£ï¼Œç®€å•å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ç§ï¼š

- å®Œå…¨åŒæ­¥(On-policy and synchronous)ï¼štrainer å’Œ explorer æ¯æ­¥è¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨æœ€æ–°çš„ experience è¿›è¡Œè®­ç»ƒã€‚
- å‘¨æœŸæ€§åŒæ­¥ï¼štrainer å’Œ explorer æ¯éš”ä¸€å®šæ­¥æ•°è¿›è¡Œä¸€æ¬¡åŒæ­¥ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨æœ€æ–°çš„ experience è¿›è¡Œè®­ç»ƒã€‚
- ðŸ’¡ One-step off-policyï¼štrainer å’Œ explorer æ¯æ­¥è¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼Œä½† explorer æ€»æ˜¯æ¯” trainer å¿«ä¸€æ­¥ï¼Œè¿™æ · trainer ä¸€ç›´åœ¨ç”¨ off-policy ç¨‹åº¦ä¸º 1 çš„ experiences è¿›è¡Œè®­ç»ƒã€‚[æ³¨]
- å®Œå…¨å¼‚æ­¥(Fully asynchronous)ï¼štrainer å’Œ explorer å®Œå…¨ç‹¬ç«‹ï¼Œtrainer æ‹¿æ¥è®­ç»ƒçš„ experiences åªç”± sample ç­–ç•¥å†³å®šã€‚

è¿˜æœ‰ä¸€ä¸ªæ¯”è¾ƒå¥½çŽ©çš„åœºæ™¯æ˜¯ï¼Œå¤šä¸ª explorers äº§ç”Ÿ experiencesï¼Œä¸€ä¸ª trainer è¿›è¡Œè®­ç»ƒï¼Œè¿™ç§è®¾å®šä¸º scale up æä¾›äº†æ–°çš„ç»´åº¦ã€‚


æ³¨ï¼šåœ¨æ—©æœŸ RL è®­ç»ƒå·¥ä½œä¸­ï¼Œå¤§å®¶ç»å¸¸å¼ºè°ƒ on-policy training çš„é‡è¦æ€§ï¼Œä½†è¿‘æœŸä¹Ÿæœ‰å·¥ä½œè¡¨æ˜Žï¼ˆæ‡’å¾—æ‰¾å‚è€ƒæ–‡çŒ®äº†ï¼‰ï¼Œoff-policy training ä¹Ÿå¯ä»¥èŽ·å¾—å¾ˆå¥½çš„æ•ˆæžœï¼Œä¹Ÿå°±æ˜¯è¯´äº§ç”Ÿ experience çš„æ¨¡åž‹å’Œæ­£åœ¨è®­ç»ƒçš„æ¨¡åž‹å¯èƒ½ä¸å®Œå…¨ç›¸åŒã€‚å®žéªŒä¸­ä¹Ÿå‘çŽ° one-step off-policy training å¯ä»¥èŽ·å¾—å¾ˆå¥½çš„æ•ˆæžœã€‚æˆ‘ä¸ªäººå¯¹äºŽ off-policy training çš„çœ‹æ³•æ˜¯ï¼šä¸€ä¸ªå¥½çš„ RL ç®—æ³•åº”è¯¥æœ‰è¶³å¤Ÿçš„ robustness æ¥åº”å¯¹ä¸ä¸¥é‡çš„ off-policy å·®å¼‚ã€‚

![Trinity-RFT è°ƒåº¦ç­–ç•¥](../../images/trinity-mode.png)


### å…¨ç”Ÿå‘½å‘¨æœŸçš„ data pipeline

æ•°æ®ï¼ˆä¸ç®¡æ˜¯ task è¿˜æ˜¯ experienceï¼‰æ˜¯ RL è®­ç»ƒçš„åŸºçŸ³ï¼Œè¿™ä¸€ç‚¹åº”è¯¥æ‰€æœ‰äººéƒ½åŒæ„ã€‚ä¸ºäº†ä¿è¯æ•°æ®çš„è´¨é‡ï¼ŒTrinity-RFT æä¾›äº†å…¨ç”Ÿå‘½å‘¨æœŸçš„ data pipelineï¼Œä»Ž task çš„ç”Ÿæˆåˆ° experience çš„æ¶ˆè€—ã€‚è¿™ä¸ªä¸œè¥¿åˆ°åº•æ˜¯å¹²ä»€ä¹ˆçš„å‘¢ï¼Ÿä½ å¯ä»¥ç†è§£ä¸ºæ¡†æž¶åœ¨æ¯ä¸ªå¡ç‚¹éƒ½è®¾ç½®äº†æ•°æ®æ“ä½œçš„é’©å­ï¼Œä½ å¯ä»¥åœ¨è¿™äº›é’©å­ä¸Šåšä¸€äº›æ•°æ®å¤„ç†çš„æ“ä½œï¼Œæ¯”å¦‚ï¼š

- Explorer ä½¿ç”¨å“ªäº› tasks è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥è®¾ç½® `task_selector.selector_type`ï¼Œç®€å•çš„æœ‰éšæœºé‡‡æ ·ï¼ˆrandomï¼‰ï¼Œæ ¹æ®å›°éš¾åº¦é‡‡æ ·ï¼ˆdifficultyï¼‰ç­‰ã€‚
- Explorer å¯¹äº§ç”Ÿçš„ experiences åšåŽå¤„ç†ï¼Œå¯ä»¥è®¾ç½® `ExperienceOperator`ï¼Œæ¯”å¦‚ç­›é€‰æŽ‰ä¸€ç»„ advantage ç›¸åŒçš„ experiencesï¼Œæˆ–è€…åªä¿ç•™æ­£æ ·æœ¬/è´Ÿæ ·æœ¬ï¼Œç”šè‡³å åŠ å¤šä¸ª operatorsã€‚
- Trainer ä½¿ç”¨å“ªäº› experiences è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥è®¾ç½® `algorithm.sample_strategy`ï¼ŒåŒ…æ‹¬æŽ§åˆ¶ off-policy ç¨‹åº¦ï¼Œé‡‡æ ·æ¯”ä¾‹ç­‰ã€‚

åŸºæœ¬ä¸Šå°±æ˜¯æ‰€æœ‰å¡ç‚¹éƒ½æä¾›äº†æ•°æ®å¤„ç†çš„æŽ¥å£ï¼Œè®©ä½ å¯ä»¥çµæ´»åœ°å¤„ç†æ•°æ®ã€‚

![Trinity-RFT æ•°æ®å¤„ç†](../../images/trinity_data_process.png)


## ä½¿ç”¨æŒ‡å—

å†™åˆ°è¿™é‡Œæˆ‘å‘çŽ°ä¸Šé¢çš„ä»‹ç»å®žåœ¨æ˜¯è¿‡äºŽæŠ½è±¡ï¼Œä¸‹é¢æˆ‘ç”¨å‡ ä¸ªå…·ä½“çš„ä¾‹å­æ¥ä»‹ç»ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ Trinity-RFTã€‚

### å‡å¦‚ä½ æƒ³å®žçŽ°ä¸€ä¸ªæ–°çš„çŽ¯å¢ƒ

æ–°çŽ¯å¢ƒçš„æ ¸å¿ƒè‚¯å®šæ˜¯ agent-environment çš„äº¤äº’ï¼Œè¿™éƒ¨åˆ†ä»£ç éœ€è¦åœ¨ workflow ä¸­å®žçŽ°ã€‚ä»¥ Alfworldï¼ˆä¸€ä¸ªæ¨¡æ‹Ÿæ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç§æ“ä½œä»»åŠ¡çš„è™šæ‹ŸçŽ¯å¢ƒï¼‰ ä¸ºä¾‹, StepWiseAlfworldWorkflow ç±»å®žçŽ°äº†ä¸€ä¸ªåŸºäºŽ Alfworld çš„äº¤äº’çŽ¯å¢ƒã€‚é¦–å…ˆï¼Œåœ¨ `__init__` æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†çŽ¯å¢ƒçš„ä¸€äº›å‚æ•°ï¼Œå¹¶ä¸”è°ƒç”¨ `self._setup_environment()` æ–¹æ³•æ¥åˆå§‹åŒ–çŽ¯å¢ƒã€‚å› ä¸ºè¿™ä¸ªä»»åŠ¡å¾ˆå®¹æ˜“è¢«æ‹†è§£ä¸ºå¤šæ­¥ä»»åŠ¡ï¼Œæˆ‘ä»¬åªéœ€è¦å®žçŽ°æ¯ä¸€æ­¥çš„äº¤äº’é€»è¾‘ï¼Œä¹Ÿå°±æ˜¯ `step` æ–¹æ³•ï¼š
```python
 def step(self, step_num: int) -> bool:
    if self.done:
        return False

    # Format observation for the model
    format_obs = format_observation(self.observation)  # type: ignore
    self.memory.append({"role": "user", "content": format_obs})

    # Get action from the model
    # è¿™é‡Œå¯ä»¥ç”¨ chat/chat_asyncï¼Œä¹Ÿå¯ä»¥ç”¨ openai çš„ api æ¥å®žçŽ°
    responses = self.model.chat(self.memory)
    response_text = responses[0].response_text
    self.memory.append({"role": "assistant", "content": response_text})
    action = parse_action(response_text)

    # Execute action in the environment
    observation, reward, done, info = self.env.step(action)

    # Update internal state and reward
    self.observation = observation
    self.done = done
    if self.done:
        self.final_reward = reward

    # Return False to stop the run if the episode is done
    return not self.done
```

å½“ä½ å®šä¹‰å¥½äº† `step` æ–¹æ³•åŽï¼Œæ¡†æž¶å…¶å®žä¸ºä½ åŒ…è£…å¥½äº†äº§ç”Ÿ experience çš„é€»è¾‘ï¼Œå®šä¹‰åœ¨ `run` æ–¹æ³•ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥è‡ªå·±ä¿®æ”¹ã€‚å…¶å®žè¿™ä¸ªäº¤äº’é€»è¾‘æ˜¯å¾ˆçµæ´»çš„ï¼Œä½ å¯ä»¥æŽ§åˆ¶æ¯ä¸€æ¬¡ rollout è¿”å›žçš„ experiencesï¼Œæ¯”å¦‚åªè¿”å›žæœ€åŽä¸€æ¡ experienceï¼Œæˆ–è€…è¿”å›žæ‹¼æŽ¥çš„ experience ï¼ˆ`AlfworldWorkflow`ï¼‰ã€‚


### å‡å¦‚ä½ æƒ³å®žçŽ°ä¸€ä¸ªæ–°çš„è®­ç»ƒç®—æ³•

å’ŒçŽ¯å¢ƒäº¤äº’ä¸åŒçš„æ˜¯ï¼Œç®—æ³•å…¶å®žæ›´å®¹æ˜“æ‹†æˆå¤šä¸ªæ¨¡å—ï¼Œæ¯”å¦‚ "advantage_fn", "policy_loss_fn", "sample_strategy", "kl_loss_fn", "entropy_loss_fn" ç­‰ï¼Œè¿˜æœ‰å‰é¢æåˆ°çš„æ•°æ®å¤„ç†çš„ operatorsã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™äº›æ¨¡å—æ˜¯å¯ä»¥åƒæ­ç§¯æœ¨ä¸€æ ·ç»„åˆä½¿ç”¨çš„ï¼Œè¿™æ ·çš„é«˜åº¦çµæ´»å¯¹äºŽç®—æ³•è®¾è®¡è€…æ˜¯éžå¸¸å‹å¥½çš„ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œæ¯”å¦‚ä½ æƒ³è¦è¯•è¯• CISPO ç®—æ³• [4]ï¼Œå®ƒå’Œ GRPO ç®—æ³•åœ¨å®žçŽ°ä¸Šå”¯ä¸€çš„åŒºåˆ«å°±æ˜¯ policy loss çš„è®¡ç®—æ–¹å¼ï¼Œä½ åªéœ€è¦å®žçŽ° `CISPOPolicyLossFn` ç±»ï¼Œç»§æ‰¿è‡ª `PolicyLossFn` ç±»ï¼Œå¹¶ä¸”å®žçŽ° `__call__` æ–¹æ³•ï¼š

```python
class CISPOPolicyLossFn(PolicyLossFn):
    def __init__(
        self,
        backend: str = "verl",
        clip_range_low: float = 1.0,
        clip_range_high: float = 0.28,
        enable_mask_clip: bool = False,
        mask_clip_range_low: float = 1.0,
        mask_clip_range_high: float = 0.28,
        loss_agg_mode: str = "token-mean",
    ) -> None:
        # omitted
        pass

    def __call__(  # type: ignore
        self,
        logprob: torch.Tensor,
        old_logprob: torch.Tensor,
        action_mask: torch.Tensor,
        advantages: torch.Tensor,
        **kwargs,
    ) -> Tuple[torch.Tensor, Dict]:
        negative_approx_kl = logprob - old_logprob
        ratio = torch.exp(negative_approx_kl)
        ppo_kl = masked_mean(-negative_approx_kl, action_mask)
        ratio_clamped = torch.clamp(
            ratio, min=1.0 - self.clip_range_low, max=1.0 + self.clip_range_high
        )

        # mask = 0 if ratio > 1.0 + self.clip_range_high and advantages > 0
        # mask = 0 if ratio < 1.0 - self.clip_range_low and advantages < 0
        # else 1
        mask = torch.ones_like(ratio)
        if self.enable_mask_clip:
            mask = torch.where(
                (ratio > 1.0 + self.mask_clip_range_high) & (advantages > 0),
                torch.zeros_like(ratio),
                mask,
            )
            mask = torch.where(
                (ratio < 1.0 - self.mask_clip_range_low) & (advantages < 0),
                torch.zeros_like(ratio),
                mask,
            )

        cispo_loss = -advantages * ratio_clamped.detach() * mask.detach() * logprob

        loss = aggregate_loss(cispo_loss, action_mask, loss_agg_mode=self.loss_agg_mode)
        unmasked_frac = masked_mean(mask, action_mask)

        metrics = {
            "cispo_loss": loss.detach().item(),
            "ppo_kl": ppo_kl.detach().item(),
            "unmasked_frac": unmasked_frac.detach().item(),
        }

        return loss, metrics
```

è®°å¾—åœ¨`algorithm/policy_loss_fn/__init__.py`ä¸­æ³¨å†Œè¿™ä¸ªç±»ï¼š
```python

POLICY_LOSS_FN: Registry = Registry(
    "policy_loss_fn",
    default_mapping={
        "cispo": "trinity.algorithm.policy_loss_fn.cispo_policy_loss.CISPOPolicyLossFn",
    },
)
```

ä¹‹åŽï¼Œå½“ä½ æƒ³è¦åˆ‡æ¢åˆ°è¿™ç§ policy loss è®¡ç®—æ–¹å¼æ—¶ï¼Œåªéœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­å¡«å†™`algorithm.policy_loss_fn=cispo`å³å¯ã€‚

å¦å¤–ï¼Œæ¡†æž¶ä¹ŸåŒ…è£…å¥½äº†å¾ˆå¤šå¸¸è§çš„è®­ç»ƒç®—æ³•ï¼Œæ¯”å¦‚ PPO, GRPO, RLOO, REINFORCE++, etc. ä½ åªéœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­å¡«å†™`algorithm.algorithm_type=ppo`å³å¯ã€‚


## æ€»ç»“

Trinity-RFT æ˜¯ä¸€ä¸ªçµæ´»ç¨‹åº¦å¾ˆé«˜çš„æ¡†æž¶ï¼Œå°¤å…¶æ˜¯æˆ‘ä¸ªäººå¾ˆå–œæ¬¢è¿™ç§å¯ä»¥ä»»æ„æ­ç§¯æœ¨çš„ç‰¹æ€§ï¼ˆablation ä¹Ÿå¾ˆæ–¹ä¾¿ï¼‰ã€‚æ¡†æž¶æœ¬èº«è¿˜åœ¨å‘å±•ä¸­ï¼Œå¸Œæœ›å¯ä»¥æœ‰æ›´å¤šçš„ç ”ç©¶è€…æ¥ä½¿ç”¨å’Œæåé¦ˆï¼ˆ[issues](https://github.com/modelscope/Trinity-RFT/issues)ï¼‰ã€‚ä¹Ÿå¸Œæœ›æˆ‘åœ¨ 2026 å¹´ä¹Ÿå¯ä»¥å¤šå†™ä¸€äº›åšå®¢ï¼Œåˆ†äº«ç›¸å…³çš„ä½¿ç”¨ç»éªŒã€‚


[1] Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models. https://arxiv.org/pdf/2505.17826

[2] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention. https://arxiv.org/pdf/2506.13585 